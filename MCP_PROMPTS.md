# MCP Conventional Prompts - Auto-Injection System

## Vue d'ensemble

Le syst√®me de prompts conventionnels permet aux serveurs MCP d'injecter automatiquement du contexte dans les conversations en exposant des prompts avec des noms sp√©cifiques. Ce syst√®me suit les conventions de nommage pour d√©terminer **o√π** et **comment** injecter chaque prompt dans la conversation.

## Architecture

### Composants principaux

1. **`src/lib/mcpPromptInjection.ts`** - Syst√®me central d'injection
2. **`src/hooks/useStreamingChat.ts`** - Int√©gration dans le flux de chat
3. **`src/lib/mcpManager.ts`** - Communication avec les serveurs MCP
4. **`electron/main.ts` + `electron/preload.ts`** - Handlers IPC pour Electron

## Noms de prompts conventionnels

Le syst√®me d√©tecte automatiquement les prompts bas√©s sur leur nom (case-insensitive):

| Nom du prompt | R√¥le OpenAI | Position | Comportement |
|---------------|-------------|----------|--------------|
| `system_prompt` | `system` | D√©but (avant tout) | Concat√©n√© avec autres system_prompt si multiples |
| `tool_instructions` | `system` | Avec system_prompt | Concat√©n√© au system prompt |
| `user_prompt` | `user` | Apr√®s system, avant conversation | Multiple autoris√©, ordre pr√©serv√© |
| `assistant_prompt` | `assistant` | Apr√®s user_prompt | Pr√©fill de r√©ponse, multiple autoris√© |
| `tool_call:*` | `assistant` + `tool` | Entre user et assistant | Simule un appel d'outil (ex: `tool_call:example1`) |
| `tool_result:*` ou `tool_answer:*` | `tool` | Apr√®s tool_call | R√©sultat d'outil simul√© |

## Ordre d'injection

Les messages sont inject√©s dans cet ordre pr√©cis:

```
1. system_prompt (tous concat√©n√©s)
2. tool_instructions (concat√©n√© au system)
3. [System prompt du thread si pr√©sent - fusionn√© avec #1]
4. user_prompt (multiples possibles, ordre pr√©serv√©)
5. tool_call + tool_result (paires, ordre pr√©serv√©)
6. assistant_prompt (multiples possibles, ordre pr√©serv√©)
7. [Messages de conversation r√©els commencent ici]
```

## Flux de fonctionnement

### 1. Exposition des prompts (C√¥t√© serveur MCP)

Un serveur MCP expose des prompts via le protocole MCP:

```typescript
// Dans un serveur MCP (ex: obsidian-memory)
server.setRequestHandler(ListPromptsRequestSchema, async () => ({
  prompts: [
    {
      name: 'user_prompt',  // ‚Üê Nom conventionnel d√©tect√©
      description: 'Main memory index',
    }
  ]
}))

server.setRequestHandler(GetPromptRequestSchema, async (request) => {
  if (request.params.name === 'user_prompt') {
    return {
      messages: [
        {
          role: 'user',
          content: {
            type: 'text',
            text: '# Memory Vault Index\n\n[Contenu de la m√©moire...]'
          }
        }
      ]
    }
  }
})
```

### 2. R√©cup√©ration et d√©tection (C√¥t√© client)

Lors de l'envoi d'un message par l'utilisateur:

```typescript
// 1. Interroger tous les serveurs MCP actifs
const servers = mcpManager.getConnectedServers()

// 2. Pour chaque serveur, lister les prompts
const prompts = await mcpManager.listPromptsFromServer(serverId)

// 3. D√©tecter les prompts conventionnels
function detectPromptType(promptName: string): ConventionalPromptType | null {
  const normalized = promptName.toLowerCase().trim()

  if (normalized === 'system_prompt') return 'system_prompt'
  if (normalized === 'user_prompt') return 'user_prompt'
  // ... autres d√©tections

  return null  // Prompt ignor√© s'il ne suit pas la convention
}

// 4. R√©cup√©rer le contenu des prompts conventionnels
const content = await mcpManager.getPromptContent(serverId, promptName)
```

### 3. Construction des messages inject√©s

```typescript
function buildInjectedMessages(prompts: ConventionalPrompt[]): OpenAIMessage[] {
  const messages: OpenAIMessage[] = []

  // 1. Fusionner tous les system_prompt
  const systemPrompts = prompts.filter(p => p.type === 'system_prompt')
  if (systemPrompts.length > 0) {
    const combinedSystem = systemPrompts
      .map(p => `[System instructions from Server: ${p.serverName}]\n${p.content}`)
      .join('\n\n---\n\n')

    messages.push({
      role: 'system',
      content: combinedSystem
    })
  }

  // 2. Ajouter user_prompt
  prompts.filter(p => p.type === 'user_prompt')
    .forEach(p => messages.push({ role: 'user', content: p.content }))

  // 3. Ajouter tool_call/tool_result
  // 4. Ajouter assistant_prompt

  return messages
}
```

### 4. Fusion avec le contexte du thread

```typescript
// Dans useStreamingChat.ts

// R√©cup√©rer les prompts inject√©s
const injectedMessages = await getInjectedMessages(mcpManager)

// Fusionner avec le system prompt du thread
const threadSystemPrompt = threadStore.currentSystemPrompt

if (threadSystemPrompt && injectedMessages[0]?.role === 'system') {
  // Ajouter le system prompt du thread aux prompts MCP
  injectedMessages[0].content += `\n\n---\n\n[Thread System Prompt]\n${threadSystemPrompt}`
} else if (threadSystemPrompt) {
  // Pas de system prompt MCP, cr√©er un nouveau
  injectedMessages.unshift({
    role: 'system',
    content: threadSystemPrompt
  })
}
```

### 5. Construction finale de la conversation

```typescript
const conversationMessages = [
  ...injectedMessages,           // Prompts MCP inject√©s
  ...userConversationMessages    // Messages utilisateur/assistant r√©els
]

// Envoi √† l'API OpenAI
await fetch(`${baseURL}/chat/completions`, {
  body: JSON.stringify({
    model: modelConfig.model,
    messages: conversationMessages,
    stream: true
  })
})
```

## Exemple concret: Obsidian Memory

### Configuration du serveur

Le serveur `obsidian-memory` expose deux prompts pour simuler un appel d'outil:

```typescript
// mcp-servers/built-in/obsidian-memory/src/index.ts
{
  name: 'tool_call:memory_index',
  description: 'Simulated tool call to retrieve the memory vault root index'
},
{
  name: 'tool_result:memory_index',
  description: 'Result of the memory_get_root tool call'
}
```

### Contenu retourn√©

**tool_call:memory_index** (Message assistant simul√©):
```
Let me check the memory vault index to understand what information is available.
```

**tool_result:memory_index** (R√©sultat JSON du tool):
```json
{
  "success": true,
  "tool": "memory_get_root",
  "data": {
    "id": "root-index",
    "title": "Index",
    "path": "_index.md",
    "content": "[Contenu markdown de la note root]",
    "links": ["Project A", "Project B", "Ideas", "Notes", "Archive"],
    "backlinks": ["Daily Notes", "Quick Capture", "Reference"],
    "tags": ["index", "root"],
    "created": "2025-01-01T00:00:00.000Z",
    "modified": "2025-01-15T12:30:00.000Z"
  }
}
```

### R√©sultat dans la conversation

Quand un utilisateur envoie un message, l'API re√ßoit:

```json
{
  "messages": [
    {
      "role": "system",
      "content": "[System prompt du thread si pr√©sent]"
    },
    {
      "role": "assistant",
      "content": "Let me check the memory vault index to understand what information is available."
    },
    {
      "role": "tool",
      "tool_call_id": "memory_index",
      "name": "memory_index",
      "content": "{\"success\":true,\"tool\":\"memory_get_root\",\"data\":{...}}"
    },
    {
      "role": "user",
      "content": "Comment puis-je organiser mes projets?"
    }
  ]
}
```

### Avantages de cette approche

üéØ **Entra√Æne le LLM** √† comprendre que:
1. Il peut appeler `memory_get_root` pour obtenir l'index
2. Le r√©sultat est structur√© en JSON avec des m√©tadonn√©es
3. Les outils de m√©moire sont disponibles et utiles
4. Il doit √™tre proactif dans l'utilisation des tools

üí° Le LLM voit un **exemple d'utilisation r√©ussie** du tool avant m√™me de commencer la conversation, ce qui l'incite √† utiliser les outils m√©moire de mani√®re plus naturelle et fr√©quente.

## Cr√©ation d'un prompt conventionnel

### √âtape 1: Ajouter la capacit√© prompts

```typescript
const server = new Server({
  name: 'mon-serveur',
  version: '1.0.0'
}, {
  capabilities: {
    tools: {},
    prompts: {}  // ‚Üê Ajouter cette ligne
  }
})
```

### √âtape 2: Impl√©menter les handlers

```typescript
// Lister les prompts disponibles
server.setRequestHandler(ListPromptsRequestSchema, async () => ({
  prompts: [
    {
      name: 'system_prompt',  // Nom conventionnel
      description: 'Configuration syst√®me de mon serveur'
    },
    {
      name: 'user_prompt',
      description: 'Contexte utilisateur automatique'
    }
  ]
}))

// Retourner le contenu d'un prompt
server.setRequestHandler(GetPromptRequestSchema, async (request) => {
  const { name } = request.params

  if (name === 'system_prompt') {
    return {
      messages: [
        {
          role: 'user',  // Le r√¥le est d√©termin√© par le nom, pas ce champ
          content: {
            type: 'text',
            text: 'Tu es un assistant sp√©cialis√© en...'
          }
        }
      ]
    }
  }

  if (name === 'user_prompt') {
    const context = await getMyContext()
    return {
      messages: [
        {
          role: 'user',
          content: {
            type: 'text',
            text: `Contexte actuel:\n${context}`
          }
        }
      ]
    }
  }

  throw new McpError(ErrorCode.InvalidRequest, `Unknown prompt: ${name}`)
})
```

### √âtape 3: Tester

1. D√©marrez le serveur MCP
2. Dans l'app Jarvis, ouvrez les d√©tails du serveur
3. Allez dans l'onglet "Prompts"
4. V√©rifiez que vos prompts apparaissent avec leur badge de type
5. D√©marrez une conversation - les prompts seront automatiquement inject√©s!

## Cas d'usage avanc√©s

### Entra√Ænement du LLM via tool calls simul√©s

**Objectif**: Montrer au LLM comment utiliser les tools de mani√®re proactive en simulant des appels r√©ussis.

**Technique**: Au lieu d'injecter directement du contexte avec `user_prompt`, utilisez une paire `tool_call` + `tool_result` pour d√©montrer l'utilisation d'un tool.

**Exemple - Obsidian Memory**:

```typescript
// Au lieu de ceci (approche passive):
{
  name: 'user_prompt',
  description: 'Memory vault index'
}
// Contenu: "Voici l'index du vault: [donn√©es]"

// Faire ceci (approche active - entra√Ænement):
{
  name: 'tool_call:memory_index',
  description: 'Simulated call to memory_get_root'
}
// Contenu: "Let me check the memory vault..."

{
  name: 'tool_result:memory_index',
  description: 'Result of memory_get_root'
}
// Contenu: {"success": true, "tool": "memory_get_root", "data": {...}}
```

**R√©sultat**: Le LLM apprend que:
- ‚úÖ Il PEUT et DEVRAIT appeler `memory_get_root` de lui-m√™me
- ‚úÖ Le format de r√©ponse attendu est du JSON structur√©
- ‚úÖ Les tools sont fiables et retournent des donn√©es utiles
- ‚úÖ C'est une bonne pratique d'√™tre proactif avec les tools

**Quand utiliser cette technique**:
- ‚úÖ Vous voulez que le LLM utilise certains tools automatiquement
- ‚úÖ Le tool retourne beaucoup de donn√©es structur√©es
- ‚úÖ Vous avez des tools "d'initialisation" (get_config, list_items, etc.)
- ‚ùå Le contexte est simple et ne n√©cessite pas d'interaction

### Prompt avec arguments

```typescript
{
  name: 'user_prompt',
  description: 'Contexte avec param√®tres',
  arguments: [
    {
      name: 'depth',
      description: 'Niveau de d√©tail (1-3)',
      required: false
    },
    {
      name: 'focus',
      description: 'Domaine de focus',
      required: true
    }
  ]
}

// Pour l'instant, les arguments ne sont pas utilis√©s automatiquement
// Le syst√®me injecte toujours sans arguments
// Feature future: configuration par utilisateur
```

### Multiples serveurs avec system_prompt

Si plusieurs serveurs exposent `system_prompt`, ils sont concat√©n√©s:

```
[System instructions from Server: obsidian-memory]
Tu as acc√®s √† une m√©moire Obsidian...

---

[System instructions from Server: code-analyzer]
Tu peux analyser du code...

---

[Thread System Prompt]
R√©ponds de mani√®re concise.
```

### Tool calls simul√©s

Pour simuler une interaction outil:

```typescript
// Serveur expose deux prompts
{
  name: 'tool_call:weather',
  description: 'Simule un appel m√©t√©o'
}
{
  name: 'tool_result:weather',
  description: 'R√©sultat m√©t√©o simul√©'
}

// R√©sultat dans la conversation:
[
  { role: 'assistant', content: '{"function":"weather","args":{"city":"Paris"}}' },
  { role: 'tool', tool_call_id: 'weather', content: '{"temp":15,"conditions":"sunny"}' }
]
```

## D√©bogage

### Logs console

Les logs suivants sont √©mis lors de l'injection:

```javascript
console.log(`[useStreamingChat] Injected ${count} conventional prompts from MCP servers`)
console.warn('[useStreamingChat] Failed to fetch conventional prompts:', error)
```

### Visualisation

1. Ouvrez DevTools (F12)
2. Onglet "Prompts" dans les d√©tails d'un serveur
3. Section "Overview" affiche le nombre de prompts
4. V√©rifiez que le badge de type est correct

### Erreurs courantes

| Erreur | Cause | Solution |
|--------|-------|----------|
| Prompt non inject√© | Nom ne suit pas la convention | Utiliser un nom exact: `system_prompt`, `user_prompt`, etc. |
| Serveur pas trouv√© | Serveur non RUNNING | V√©rifier le statut dans la liste MCP |
| Contenu vide | GetPrompt retourne vide | V√©rifier l'impl√©mentation du handler |
| Ordre incorrect | Mauvais type d√©tect√© | Respecter case-insensitive: `System_Prompt` = `system_prompt` |

## Limitations actuelles

1. **Pas d'arguments dynamiques** - Les prompts sont appel√©s sans arguments
2. **Pas de d√©sactivation s√©lective** - Tous les prompts conventionnels sont inject√©s
3. **Pas de cache** - Prompts r√©cup√©r√©s √† chaque message
4. **Pas de preview utilisateur** - L'injection est automatique et invisible

## √âvolutions futures

- [ ] Configuration utilisateur pour activer/d√©sactiver certains prompts
- [ ] Support des arguments avec UI de configuration
- [ ] Cache des prompts avec invalidation intelligente
- [ ] Preview du contexte inject√© dans l'interface
- [ ] M√©triques de tokens utilis√©s par les prompts
- [ ] Priorit√© et ordre personnalisable
- [ ] Conditions d'injection (selon le thread, le mod√®le, etc.)

## R√©f√©rences

- **Sp√©cification MCP**: https://modelcontextprotocol.io/specification
- **Code source**:
  - `src/lib/mcpPromptInjection.ts` - Syst√®me d'injection
  - `src/hooks/useStreamingChat.ts` - Int√©gration chat (lignes 190-239)
  - `mcp-servers/built-in/obsidian-memory/src/index.ts` - Exemple serveur

---

**Note**: Ce syst√®me est une extension du protocole MCP standard. Les prompts conventionnels sont une convention de nommage sp√©cifique √† Jarvis pour faciliter l'injection automatique de contexte.
